---
title: In search of the sexiest map
---

I spent today working on a [Stupid Hackathon](http://stupidhackathon.github.io) idea, shamelessly stolen from [Tom Brown](http://nottombrown.com), that sadly did not get built at the event itself.

The Google Vision API provides an interface for pushing images through Google's Safe Search algorithm, which returns the likelihood that the image contains adult, violent, medical, or spoof content. By testing thousands of satellite images, we can find what Google believes to be the sexiest map.

Because [Planet Labs](http://planet.com) requires registration for access to their full [Open California](http://planet.com/open-california/#) data, I had to scrape what I could from their public Google Maps style interface. I wrote a Python script to take 4 tiles from zoom level 14, and combine them into one 512x512 pixel image. By themselves, the images are frequently abstractly beautiful:

![Satellite image 1]({{site.baseurl}}/images/2016-05-20/427.png)

![Satellite image 1]({{site.baseurl}}/images/2016-05-20/439.png)

![Satellite image 1]({{site.baseurl}}/images/2016-05-20/21.png)

I scraped about 5000 of these, before feeding them to the Google Vision API.

Unfortunately, though I had thought that the Google Vision API would return a float likelihood value for each of the categories, it actually abstracts that into several qualitative likelihood values, ranging from "very unlikely" to "very likely."

Of the 5000 images, Google found two considered "likely" to have adult content, and one considered "likely" to have violent content. Are you ready for the two sexiest satellite images of California?

![Sexy satellite image 1]({{site.baseurl}}/images/2016-05-20/1189.png)

![Sexy satellite image 2]({{site.baseurl}}/images/2016-05-20/1193.png)

I don't really understand either, but whatever floats your boat, Google.

And, the satellite image Google thought most likely to contain violent content:

![Violent satellite image]({{site.baseurl}}/images/2016-05-20/4512.png)

This one maybe makes more sense, but it's still pretty hard to imagine.

Google also found several images that "possibly" contained unsafe content: 74 with violence, 27 with medical content, and 9 with adult content.

Of these, most of the adult images were also views of rectangular farms and fields, sometimes with rivers snaking through them. I wonder if Google is using something kind of old-school for this, like a [connected-element/limb analyzer](http://homes.cs.washington.edu/~shapiro/EE596/notes/forsyth.pdf)?

![Possibly sexy satellite image]({{site.baseurl}}/images/2016-05-20/1277.png)

![Possibly sexy satellite image]({{site.baseurl}}/images/2016-05-20/1293.png)

![Possibly sexy satellite image]({{site.baseurl}}/images/2016-05-20/1284.png)

> Maybe, if you squint.

The most understandable mistakes were the images that Google categorized as possibly containing medical content. Who can forget the rolling, golden skin of California, with its stretching unpaved wrinkles connecting dry ranches to dusty market towns?

![Possibly medical satellite image]({{site.baseurl}}/images/2016-05-20/1929.png)

![Possibly medical satellite image]({{site.baseurl}}/images/2016-05-20/816.png)

![Possibly medical satellite image]({{site.baseurl}}/images/2016-05-20/1883.png)

![Possibly medical satellite image]({{site.baseurl}}/images/2016-05-20/1794.png)

Good job, Google. Good job.

---

Code for scraping Planet Labs images:

{% gist 3ddda4bdbd507fc06037931424b8010e %}

Code for talking with the Google Vision API, based on Google's [Text Detection starter code](https://github.com/GoogleCloudPlatform/cloud-vision/tree/master/python/text):

{% gist 5861ffeaf325516ed9f9a18ed82b161b %}